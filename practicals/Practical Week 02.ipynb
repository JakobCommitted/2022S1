{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration\n",
    "\n",
    "The following demonstration will use the training set of the OHSUMED corpus. This training set was used in the Filtering Track of the 9th edition of the Text REtrieval Conference (TREC-9). We will use it for the information retrieval exercises of this workshop. Download [ohsumed.zip](ohsumed.zip) into the same folder as this notebook. The file is part of the git repository, so if you have cloned or downloaded the entire repository you will have the file in the right folder.\n",
    "\n",
    "The following code unzips the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('ohsumed.zip', 'r')\n",
    "zip_ref.extractall('.')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you read the data, we are providing the file ohsumed.py (in the zip file above) that has a simple API to the data. When you import it at the Python prompt, it will provide the following variables:\n",
    "\n",
    "\n",
    "1. `index`: a dictionary with document IDs as keys, and document text as values.\n",
    "2. `questions`: a dictionary with query IDs as keys, and query text as values.\n",
    "3. `answers`: a dictionary with query IDs as keys, and a set with the IDs of known relevant documents as values. This information is used for evaluation.\n",
    "\n",
    "Below are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading OHSUMED data\n"
     ]
    }
   ],
   "source": [
    "import ohsumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54710"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohsumed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['87049087',\n",
       " '87049088',\n",
       " '87049089',\n",
       " '87049090',\n",
       " '87049091',\n",
       " '87049092',\n",
       " '87049093',\n",
       " '87049094',\n",
       " '87049095',\n",
       " '87049096']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(ohsumed.index.keys()))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Serum lipids and lipoproteins were examined in a group of 45 healthy postmenopausal women who were treated for 2 years with either 3 mg of percutaneous estradiol (n = 20) or placebo (n = 25). Percutaneous estradiol was given alone during the first year of treatment and in combination with oral micronized progesterone (200 mg) for 12 days of each cycle during the second year. The women were examined every 3 months throughout the 2 years. Percutaneous estrogen therapy significantly reduced total serum cholesterol and low-density lipoprotein cholesterol, whereas no significant differences were observed in serum triglycerides and high-density lipoprotein cholesterol. Addition of oral progesterone during the second year of treatment did not produce any significant alterations in serum total cholesterol or low-density lipoprotein cholesterol, both of which remained significantly reduced. Serum triglycerides remained virtually unchanged, whereas a slight but significant increase (p less than 0.05) was observed in high-density lipoprotein cholesterol levels at the end of the study period. We conclude that percutaneous estrogen administration produces changes in total serum cholesterol and low-density lipoprotein cholesterol levels similar to those observed after oral estrogen administration. However, the magnitude and time course of the response seem to be modulated by the route of administration. Addition of oral micronized progesterone does not influence the beneficial estrogenic actions on serum lipids and lipoproteins and seems to be a proper \"progestogen\" in percutaneous estrogen therapy.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohsumed.index['87097544']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohsumed.questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OHSU1',\n",
       " 'OHSU10',\n",
       " 'OHSU11',\n",
       " 'OHSU12',\n",
       " 'OHSU13',\n",
       " 'OHSU14',\n",
       " 'OHSU15',\n",
       " 'OHSU16',\n",
       " 'OHSU17',\n",
       " 'OHSU18']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(ohsumed.questions.keys()))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'60 year old menopausal woman without hormone replacement therapy Are there adverse effects on lipids when progesterone is given with estrogen replacement therapy'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohsumed.questions['OHSU1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohsumed.answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'87097544', '87157536', '87157537', '87202778', '87316316', '87316326'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohsumed.answers['OHSU1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index\n",
    "\n",
    "We are going to build an inverted index of the non-stop words with frequency higher than 5.\n",
    "\n",
    "The following code reads the files and creates a counter of all words in the corpus (including stop words). We will use NLTK's word tokeniser (read the beginning of [chapter 3 of NLTK's book](http://www.nltk.org/book/ch03.html#processing-raw-text)) to convert each document into a list of tokens. **Note that this code may take some time to run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jakob/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jakob/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, collections\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "wordcounter = collections.Counter([w.lower() for k in ohsumed.index\n",
    "                                             for s in nltk.sent_tokenize(ohsumed.index[k])\n",
    "                                             for w in nltk.word_tokenize(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 305806),\n",
       " ('of', 271953),\n",
       " ('.', 254858),\n",
       " (',', 239656),\n",
       " ('and', 179604),\n",
       " ('in', 172449),\n",
       " ('to', 107431),\n",
       " (')', 96259),\n",
       " ('(', 95948),\n",
       " ('a', 95281)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the inverted index of all non-stop words with frequency higher than 5. **Note that this code  may take some time to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted = dict()\n",
    "for d in ohsumed.index:\n",
    "    for w in nltk.word_tokenize(ohsumed.index[d]):\n",
    "        w = w.lower()\n",
    "        if w in stop or wordcounter[w] <= 5:\n",
    "            continue\n",
    "        if w in inverted:\n",
    "            inverted[w].add(d)\n",
    "        else:\n",
    "            inverted[w] = set([d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patients', 'converted', 'ventricular', 'fibrillation', 'organized', 'rhythms', 'ambulance', 'technicians', '(', ')', 'hospital', 'arrival', '.', 'authors', 'analyzed', '271', 'cases', 'managed', 'working', 'without']\n"
     ]
    }
   ],
   "source": [
    "print(list(inverted.keys())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerates',\n",
       " 'accelerating',\n",
       " 'acceleration',\n",
       " 'accelerations',\n",
       " 'accelerator',\n",
       " 'accentuate',\n",
       " 'accentuated',\n",
       " 'accentuation',\n",
       " 'accept',\n",
       " 'acceptability',\n",
       " 'acceptable',\n",
       " 'acceptably',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'acceptor',\n",
       " 'acceptors']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(inverted.keys()))[2990:3010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'87057543',\n",
       " '87067994',\n",
       " '87073895',\n",
       " '87074134',\n",
       " '87114326',\n",
       " '87119697',\n",
       " '87121859',\n",
       " '87129900',\n",
       " '87149032',\n",
       " '87153185',\n",
       " '87193350',\n",
       " '87223625',\n",
       " '87223856',\n",
       " '87224779',\n",
       " '87232524',\n",
       " '87251875',\n",
       " '87273001',\n",
       " '87282178',\n",
       " '87295871',\n",
       " '87297008'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted['acceptability']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code saves the inverted index into a pickle file. This way we do not need to compute the inverted index again. Read [Python's documentation on pickle files](https://docs.python.org/3/library/pickle.html) for more detail. Note that the file we created is opened for writing in binary mode, following the advice of this [stackoverflow post about saving pickle files](http://stackoverflow.com/questions/13906623/using-pickle-dump-typeerror-must-be-str-not-bytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('inverted.pickle', 'wb') as f:\n",
    "    pickle.dump(inverted,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads the pickle file and returns the list of documents that maches this Boolean query:\n",
    "\n",
    "1. (menopausal OR pregnant) AND woman AND NOT healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('inverted.pickle', 'rb') as f:\n",
    "    inverted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'87060673',\n",
       " '87066899',\n",
       " '87097274',\n",
       " '87097518',\n",
       " '87099263',\n",
       " '87114245',\n",
       " '87117852',\n",
       " '87128881',\n",
       " '87134330',\n",
       " '87138205',\n",
       " '87153548',\n",
       " '87153568',\n",
       " '87169457',\n",
       " '87185313',\n",
       " '87226668',\n",
       " '87231479',\n",
       " '87235637',\n",
       " '87251241',\n",
       " '87252385',\n",
       " '87261426',\n",
       " '87281235',\n",
       " '87290433',\n",
       " '87296136',\n",
       " '87316210',\n",
       " '87316220',\n",
       " '87316328',\n",
       " '87324028',\n",
       " '87325497'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inverted['menopausal'] | inverted['pregnant']) & inverted['woman'] - inverted['healthy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it took very little time to run the query. In general, creating the index may take some time but it is needed only once if the files do not change. Queries on the index are very fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "\n",
    "## 1. Vector Retrieval\n",
    "\n",
    "### Exercise 1.1: Boolean Information Retrieval\n",
    "\n",
    "Create an inverted index of the **NLTK Gutenberg corpus** and save it into a file \"gutenbergindex.pickle\". To create this index there is no need to look for stop words or word frequencies, since the corpus is not that large. Simply use all the words. Use this index to find the documents that match the following Boolean queries:\n",
    "\n",
    "1. Brutus OR Caesar\n",
    "2. Brutus AND NOT Caesar\n",
    "3. (Brutus AND Caesar) OR Calpurnia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /Users/jakob/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving index into file gutenbergindex.pickle...\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "nltk.download(\"gutenberg\")\n",
    "\n",
    "gutenberg=nltk.corpus.gutenberg\n",
    "guten_ids=list(gutenberg.fileids())\n",
    "\n",
    "gutenberg_index= dict()\n",
    "for doc in guten_ids:\n",
    "    words = gutenberg.words(doc)\n",
    "    for w in words:\n",
    "        if w in gutenberg_index:\n",
    "            gutenberg_index[w].add(doc)\n",
    "        else:\n",
    "            gutenberg_index[w]=set([doc])\n",
    "\n",
    "print(\"Saving index into file gutenbergindex.pickle...\")\n",
    "with open(\"gutenberg_index.pickle\",\"wb\") as f:\n",
    "    pickle.dump(gutenberg_index,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gutenberg_index.pickle','rb') as z:\n",
    "    gutenberg_index = pickle.load(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bible-kjv.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code for searching for Brutus OR Caesar\n",
    "gutenberg_index['Brutus'] | gutenberg_index['Caesar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'King', 'James', 'Bible', ']', 'The', ...]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.words('bible-kjv.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code for searching for Brutus AND NOT Caesar\n",
    "gutenberg_index['Brutus'] - gutenberg_index['Caesar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key not found in Gutenberg Index 'Calpurnia'\n"
     ]
    }
   ],
   "source": [
    "# Write your code for searching for (Brutus AND Caesar) OR Calpurnia\n",
    "try:\n",
    "    (gutenberg_index['Brutus'] & gutenberg_index['Caesar']) | gutenberg_index['Calpurnia']\n",
    "except KeyError as key:\n",
    "    print('Key not found in Gutenberg Index',key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: tf.idf\n",
    "\n",
    "Using scikit-learn, compute the tf.idf of all words in the OHSUMED corpus. Use the English list of stop words, and leave all other settings to their default values. In particular, do not stem the words. Pickle the resulting tf.idf vectoriser into a file tfidf.pickle. **Note that in this exercise you should use the sklearn functions, not nltk. In particular, do not use NLTK's list of stop words or its tokeniser.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code to compute the tf.idf\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf.fit([ohsumed.index[k] for k in ohsumed.index])\n",
    "feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code to save the results in a pickle file\n",
    "with open(\"tfidf.pickle\",\"wb\") as f:\n",
    "    pickle.dump(tfidf,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Sort by tf.idf\n",
    "\n",
    "Write a program that returns the words of a document with highest tf.idf score. The resulting list of words should be sorted by frequency in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_tfidf(tfidf, docID, numwords=10):\n",
    "    \"\"\"Print the words with highest tf.idf, in descending order\n",
    "    >>> best_tfidf(tfidf, '87049087', numwords=3)\n",
    "    ['rhythms', 'refibrillation', 'organized']\n",
    "    \"\"\"\n",
    "    # Write your code here\n",
    "    words = list(tfidf.get_feature_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rhythms',\n",
       " 'refibrillation',\n",
       " 'organized',\n",
       " 'refibrillated',\n",
       " 'converted',\n",
       " 'emt',\n",
       " 'paramedic',\n",
       " 'ds',\n",
       " 'defibrillation',\n",
       " 'hospital']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_tfidf(tfidf,'87049087')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise: tf.idf cosine similarity\n",
    "\n",
    "Use the OHSUMED collection for the following exercise. Write a function that takes as a parameter a string and an optional parameter $n$ the number of results, and returns the IDs of the $n$ documents that are most relevant according to tf.idf and cosine similarity. The results are sorted in descending order of the cosine similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following funcion implements cosine similarity by using the formulas we have seen in the lectures.\n",
    "# Feel free to use sklearn's implementation of cosine similarity instead.\n",
    "\n",
    "def best_documents(querystring,n=10):\n",
    "    \"\"\"Return the indices of the best n documents using cosine similarity\n",
    "    >>> best_documents(ohsumed.questions['OHSU1'], n=3)\n",
    "    ['87285549', '87162574', '87068356']\"\"\"\n",
    "    # Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['87052846',\n",
       " '87053030',\n",
       " '87057603',\n",
       " '87057561',\n",
       " '87054719',\n",
       " '87053640',\n",
       " '87053630',\n",
       " '87055106',\n",
       " '87057550',\n",
       " '87053614']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_documents(ohsumed.questions['OHSU1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('tensorwatchtest': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
